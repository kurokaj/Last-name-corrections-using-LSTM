{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \nimport os\nimport re\nimport random\nfrom tqdm import tqdm, trange\n\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Input,LSTM,Dense,Bidirectional \n\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# File loading\ndf  = pd.read_excel('../input/finnish-last-names-20200206/sukunimitilasto-2020-02-06.xlsx',0)\nprint(df.shape)\ndf.dropna(axis=0,how='any')\nprint(df.shape)\nprint(df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [x for x in df['Sukunimi'] if type(x) == type('a') ]\nprint(\"Name Count:\",len(names))\nprint(names[:4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\ndef process(sent):\n    sent=sent.lower()\n    # Data set is good enough so this is not needed\n    sent=re.sub(r'[^-ü0-9a-öA-Ö ]','',sent)\n    sent=re.sub(r'[ç]','c',sent)\n    sent=re.sub(r'[ê]','e',sent)\n    sent=sent.replace('\\n','')\n    return sent    \n\n# [^0-9a-öA-Ö ] = Chage everything except 0-9 and a-ö\n# Test preprocessing\na = \"Von K0rhönúûünå-Virtánéênç\"\nb = process(a)\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names =[process(x) for x in names]\ntemp = []\nfor name in names:\n    temp+= [ x for x in name.split() ]\n# Find dublicates (Some last names can include one Ala-aho -> Matala-aho)\nnames = list(set(temp))\nprint(\"\\n\".join(names[:4]))\nprint(\"Number of items:\",len(names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CHAR INDEXING\nchar_set = list(\" aábcdéefghijklmnóopqrstüuvwxyzåäö0123456789-\")\nchar2int = { char_set[x]:x for x in range(len(char_set)) }\nint2char = { char2int[x]:x for x in char_set }\nprint(char2int)\nprint(int2char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = len(char_set)\ncodes = [\"\\t\",\"\\n\",'#']\nfor i in range(len(codes)):\n    code = codes[i]\n    char2int[code]=count\n    int2char[count]=code\n    count+=1\nprint(char2int)\nprint(int2char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#thresh - 0 to 1\n# Create false last name \ndef gen_gibberish(name,thresh=0.5):\n    times = int(random.randrange(1,len(name)) * thresh)\n    '''\n    Types of replacement:\n        1.Delete random character.\n        2.Add random character.\n        3.Replace a character.\n        4.Combination?\n    '''\n    while times!=0:\n        # try to gen noise length times...\n        times-=1\n        val = random.randrange(0,10)\n        if val <= 5:\n            #get random index\n            val = random.randrange(0,10)\n            index = random.randrange(2,len(name))\n            if val <= 6 :\n                #delete character\n                name = name[:index]+name[index+1:]\n            else:\n                #add character\n                insert_index = random.randrange(0,len(char_set))\n                name = name[:index] + char_set[insert_index] + name[index:]\n        else:\n            #add character\n            index = random.randrange(0,len(char_set))\n            replace_index = random.randrange(2,len(name))\n            name = name[:replace_index] + char_set[index] + name[replace_index+1:]\n    return name\n\nsample = names[6]\ngib = gen_gibberish(sample)\nprint(\"Original:\",sample)\nprint(\"Gibberish:\",gib)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataset\ninput_texts = []\ntarget_texts = []\nvalidation_texts = []\nval_target_texts = []\nREPEAT_FACTOR = 33\n\nfor name in names:\n    output_text = '\\t' + name + '\\n'\n    # Create the training instance \n    for _ in range(REPEAT_FACTOR):\n        input_text = gen_gibberish(name)\n        input_texts.append(input_text)\n        target_texts.append(output_text)\n    # Create the testing instance \n    val_text = gen_gibberish(name)\n    validation_texts.append(val_text)\n    val_target_texts.append(output_text)\n    \nprint(\"LEN OF SAMPLES:\",len(input_texts))\nprint(\"LEN OF VALIDATION SAMPLES:\", len(validation_texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_enc_len = max([len(x) for x in input_texts])\nmax_dec_len = max([len(x) for x in target_texts])\nprint(\"Max Enc Len:\",max_enc_len)\nprint(\"Max Dec Len:\",max_dec_len)\n\n# For the validation\nval_enc_len = max([len(x) for x in validation_texts])\nval_dec_len = max([len(x) for x in val_target_texts])\nprint(\"Max VAL Enc Len:\",val_enc_len)\nprint(\"Max VAL Dec Len:\",val_dec_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_samples = len(input_texts)\nnum_val_samples = len(validation_texts)\nencoder_input_data = np.zeros( (num_samples , max_enc_len , len(char_set)),dtype='float32' )\ndecoder_input_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\ndecoder_target_data = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n\n# For the validation\nval_encoder_input_data = np.zeros( (num_val_samples , max_enc_len , len(char_set)),dtype='float32' )\nval_decoder_input_data = np.zeros( (num_val_samples , max_dec_len , len(char_set)+2),dtype='float32' )\nval_decoder_target_data = np.zeros( (num_val_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n\nprint(\"CREATED ZERO VECTORS\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filling in the enc,dec datas\n    # Encode the chracters to binary ints based on dictionary\nfor i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n    for t,char in enumerate(input_text):\n        encoder_input_data[ i , t , char2int[char] ] = 1\n    for t,char in enumerate(target_text):\n        decoder_input_data[ i, t , char2int[char] ] = 1\n        if t > 0 :\n            decoder_target_data[ i , t-1 , char2int[char] ] = 1\n            \n# For the validation set \nfor i,(validation_text,val_target_text) in enumerate(zip(validation_texts,val_target_texts)):\n    for t,char in enumerate(validation_text):\n        val_encoder_input_data[ i , t , char2int[char] ] = 1\n    for t,char in enumerate(val_target_text):\n        val_decoder_input_data[ i, t , char2int[char] ] = 1\n        if t > 0 :\n            val_decoder_target_data[ i , t-1 , char2int[char] ] = 1\n            \nprint(decoder_target_data.shape)\nprint(val_decoder_target_data.shape)\nprint(\"COMPLETED...\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\n#batch_size = 1024\nepochs = 20\nlatent_dim = 256\n\nnum_enc_tokens = len(char_set)\nnum_dec_tokens = len(char_set) + 2 # includes \\n \\t\n\n# THE ENCODER \nencoder_inputs = Input(shape=(None,num_enc_tokens))\nencoder = LSTM(latent_dim,return_state=True)\nencoder_outputs , state_h, state_c = encoder(encoder_inputs)\nencoder_states = [state_h,state_c]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# THE DECODER \ndecoder_inputs = Input(shape=(None,num_dec_tokens))\ndecoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True)\ndecoder_ouputs,_,_ = decoder_lstm(decoder_inputs,initial_state = encoder_states)\n\ndecoder_dense = Dense(num_dec_tokens, activation='softmax')\ndecoder_ouputs = decoder_dense(decoder_ouputs)\n\nmodel = Model([encoder_inputs,decoder_inputs],decoder_ouputs)\n\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h=model.fit([encoder_input_data,decoder_input_data],decoder_target_data\n         ,epochs = epochs,\n          batch_size = batch_size,\n          validation_data=([val_encoder_input_data,val_decoder_input_data],val_decoder_target_data)\n         )\nmodel.save('s2s.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# Testing the whole model, not just the encoder/decoder part\n    # Not working properly (the dense layer produces wrong words)\ntext = \"h3ikkinen\"\ninput_name = np.zeros( (1 , max_enc_len , len(char_set)),dtype='float32' )\ninput_decoder_name = np.zeros( (num_samples , max_dec_len , len(char_set)+2),dtype='float32' )\n\nfor t,char in enumerate(text):\n    input_name[ 0 , t , char2int[char] ] = 1\nfor t,char in enumerate(text):\n    input_decoder_name[ 0 , t , char2int[char] ] = 1\n    \n# The probabilities for output tokens (characters)\npredictions = model.predict([input_name,input_decoder_name]) \npredictions = np.squeeze(predictions, axis=0) \nprint(predictions.shape)\n\nword = ''\nfor idx,char in enumerate(predictions):\n    max_index = np.argmax(char, axis=0)\n    \n    # sample the character \n    sampled_char = int2char[max_index]\n    word += sampled_char\n\n    if (sampled_char == '\\n' or len(word) > max_dec_len):\n        break\n    \nprint(word)\n\nprob = np.max(predictions, axis=1)\nprint(np.average(prob))","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\ndecoder_outputs,state_h,state_c = decoder_lstm(\n        decoder_inputs,initial_state = decoder_states_inputs\n)\ndecoder_states = [state_h,state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states\n)\nencoder_model.save('encoder.h5')\ndecoder_model.save('decoder.h5')\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_dec_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, char2int['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = int2char[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_dec_len):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_dec_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\n# Loop and wait for the user to input last names\nnew_name = ''\nwhile new_name != 'quit':\n    # Ask the user for a name.\n    new_name = input(\"Input a last name or enter 'quit': \")\n    new_name=new_name.lower()\n    \n    input_name = np.zeros( (1 , max_enc_len , len(char_set)),dtype='float32' )\n    for t,char in enumerate(new_name):\n        input_name[ 0 , t , char2int[char] ] = 1\n    \n    decoded_name = decode_sequence(input_name)\n    \n    decoded_name = decoded_name.split('\\n')[0]\n    \n    if new_name == decoded_name:\n        print(\"The name is correct!\")\n    else:\n        print(\"You wrote: \",new_name, \", Did you mean: \", decoded_name)\n    \n    print(\"-\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through validation set and count the binary accuracy\ntrue = 0\namount = 0\n\nfor idx in range(val_encoder_input_data.shape[0]):\n    name = val_encoder_input_data[idx:idx+1]\n    decoded_n = decode_sequence(name)\n    \n    decoded_n = \"\\t\" + decoded_n \n    \n    if decoded_n == val_target_texts[idx]:\n        true += 1\n    amount += 1\n    \n    if not(idx % 1000):\n        print(idx, \" validation names processed...\")\n    \nacc = true/amount\nprint(\"The binary accuracy of the model: \", acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}